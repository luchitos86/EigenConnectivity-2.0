\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{amsmath}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Elsevier \LaTeX\ template\tnoteref{mytitlenote}}
\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Elsevier\fnref{myfootnote}}
\address{Radarweg 29, Amsterdam}
\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
\ead[url]{www.elsevier.com}

\author[mysecondaryaddress]{Global Customer Service\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{support@elsevier.com}

\address[mymainaddress]{1600 John F Kennedy Boulevard, Philadelphia}
\address[mysecondaryaddress]{360 Park Avenue South, New York}

\begin{abstract}
This template helps you to create a properly formatted \LaTeX\ manuscript.
\end{abstract}

\begin{keyword}
\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
\MSC[2010] 00-01\sep  99-00
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Methods}
\subsection{Subjects}
\subsection{Data acquistion}
\subsection{Data preprocessing}

\subsection{Dynamic FC estimation}
Pearson correlation was used to estimate dynamic FC region by region time-courses, yielding a correlation matrix $88 \times 88$ for each window.
Indeed, the sliding window correlation between the time series $x$ and $y$ was given by $\rho = \mbox{corr}(\ x[t,t+\Delta t], y[t,t+\Delta t] \ )$ where $\Delta t$ represents the window length in TRs and each window was then shifted by two.
We also normalized each subject by subtracting the mean over the time and dividing by its standard deviation.
We finally obtained a dynamic FC with 205 frames per subject.
Then, we concatenated all the subjects in order to perform the demeaning step by removing the global between subjects and time. The dataset is therefore described as $\mathcal{W}^s_t = \left \{ W^s_{t} \right  | t = 1...T \ | s = 1,...,N \}$, where $T$ is the number of windows and $N$ the number of subjects.  


\subsection{Joint FC connectivity estimation}
Aiming at characterizing connectivity patterns across time and subjects, we applied a joint diagonalization of Laplacian graphs to describe in a compact way the dynamic FC.\\
First we derived the corresponding Laplacian graph from each frame $t$ and subject $s$ of the dynamic FC. We used the Normalized Graph Laplacian, defined as follow:
\begin{equation}
L^s{_t} = D{^s}_{t}^{-1/2}(D{^s}_t -W{^s}_t)D{^s}_{t}^{-1/2}
\end{equation}
where $D^s_t$ is the diagonal degree matrix  and $W^s_t$ is the connectivity matrix of each frame for subject $s$. Laplacian graphs have some important properties which allow us to define a joint representation of FC across subject and time.
Basically, a Laplacian is a semi-positive definite matrix and its corresponding eigenvalues are all positive and the first is zero.
Related to graph analysis, the spectral properties of Laplacians allow to define the partition of a graph (i.e. community) using the smallest eigenvectors of the eigenspace combined with k-means clustering.\\
Aiming at deriving dFC patterns, we take into account the commutability property of two or more Laplacians which allows to find a set of shared orthonormal vectors. In principle it would be possible to build a joint eigenspace that it should represent the common patterns across multiple view.
However, in the real scenario Laplacians do not commute; however an approximation of shared vectors can be found through a joint diagonalization procedure.\\


The problem can be formulated as optimization problem solving: 
\begin{equation}
\min_{V} \sum_{{i=1}}^T  \mbox{\textbf{off}} \ {(V'L^s_{t} V)} \ ,  \quad V'V = I
\label{eqn:Jade}
\end{equation}
where \textbf{off}$(A)$ = $\sum_{i \ne j } | a_{ij}|^2$. In order to find the joint eigenspace, we applied the generalized Jacobi angles algorithm proposed by Cardoso et al. %\cite{SC-siam}.

The joint diagonalization can be achieved building a matrix %$\textbf{R}_{\theta}$ 
as a product of plane rotation globally applied to all matrices $L^s_{t}$. Through this optimization we obtained a joint eigenspace, which is composed by a set of shared orthonormal eigenvectors. 
Representing the dynFC with a joint eigenspace allows us to have a set of shared eigenconnectivity (no more than the graph dimensionality) but at the same time it allows us to retrieve the single subject contribution computing the new eigenvalues for each frame and subject. 
Basically it is possibile to estimate the new eigenvalue from the joint eigenspace and the original graph Laplacian as follow:
%\cite{SC-siam}.
\begin{equation}
\Lambda^s = \mbox{diag}(V'L^s_{t}V)  
\label{eqn:Ltilde}
\end{equation}
where $\Lambda^s$ is:

\begin{equation}
\Lambda^s = 
\begin{bmatrix} 
\lambda^s_{1,1} & \cdots  \cdots \cdots & \lambda^s_{1,T} \\ \vdots & \vdots & \vdots \\ \lambda^s_{n,1} & \cdots \cdots \cdots & \lambda^s_{n,T}  
\end{bmatrix}
\end{equation}
where $n$ is the number of nodes and $T$ the number of windows. Indeed, we obtained a timeseries for each eigenvalue which describes the evolution of the corresponding eigenconnectivity over the time. 

\begin{figure}[b!]
\includegraphics[width=0.5\textwidth]{img/sub1Eig.png}\includegraphics[width=0.5\textwidth]{img/sub18Eig.png}
\caption{\textbf{Left}: Eigenvalues timeseries of a random Healthy subject. \textbf{Right}: Eigenvalues timeseries of a random RRMS subject}
\end{figure}

\subsection{Functional Connectivity Classification}
\label{class}
The eigenvalues timeseries show how pattern of functional connectivity can occur over the time. However, in order to discriminate healthy and RRMS subjects, we need to extract time-independent features. Therefore, we used the mean and the standard deviation of each eigenvalues obtaining a feature vector $f$ of dimension $1\times 2n$, where $n$ is the number of eigenvalues.\\
Classification was performed using support vector machine (SVM) classifier and linear kernel. We first normalized the features by removing the mean and dividing by its standard deviation and then we adopted linear kernel-SVM classifier with Leave one Cross validation.
We indeed trained $N$ classifiers testing one subject per time. We also performed parameter tuning to reach the highest value in classification accuracy. \\
For identifying significantly discriminative functional patterns, we adopted bootstrapping technique which enables identification of the more stable discriminative patterns. We used classifier weights and we computed the normalized mean over bootstrap samples: $\frac{1}{N} \sum_{i=1}^N w_i^p/\mbox{std}(w_i^p), \quad \mbox{with} \quad p=1,...,2n$.\\
After bootstrapping we computed the z-score to evaluate the which connectivity patterns are significantly different for $z \ge 1.96$ and $z \le -1.96$ ($p \le 0.05$).\\
Once discriminative connectivity patterns have been recognized through the bootstrapping analysis, we map these patterns computing the outer product between the corresponding eigenvector estimated with the joint diagonalization framework.
In this way we obtain a rank one matrix for each eigenvector which should approximate the most important information. 

\section{Results}





\section{Bibliography styles}



\section*{References}

\bibliography{mybibfile}

\end{document}\grid
